PART 1 ANSWER:

Question 3.4: 
    We are given the following corpus, modified from the one in the chapter:
    <s> I am Sam </s>
    <s> Sam I am </s>
    <s> I am Sam </s>
    <s> I do not like green eggs and Sam </s>
    Using a bigram language model with add-one smoothing, what is P(Sam |
    am)? Include <s> and </s> in your counts just like any other token.

Answer:
    words: I | am |Sam | do | not | like | green | eggs | and | <s> | </s>
    count: 4 | 3  | 4  | 1  | 1   |  1   |  1    |  1   | 1   | 4   |   4

    count(am Same) / count(am) = 2/3 
    With add one smoothing: (2+1) / (11+3) => 3/14
    Answer is (3/14)


PART 2 ANSWERS:

Question 1:
  How many word types (unique words) are there in the training corpus? Please include
  the end-of-sentence padding symbol </s> and the unknown token <unk>. Do not include the start of sentence padding symbol <s>.

Answer 1:
  The number of unique words in the training corpus (minus the <s> (1)token) is: 41738

Question 2:
  How many word tokens are there in the training corpus? Do not include the start of sentence padding symbol <s>.

Answer 2:
  There are 2468210 word tokens in the training corpus 

Question 3:
  What percentage of word tokens and word types in the test corpus did not occur in training (before you mapped the unknown words to <unk> in training and test data)? Please include the padding symbol </s> in your calculations. Do not include the start of sentence padding symbol <s>.

Answer 3:
  The percentage of tokens that didnt occur is: 1.6033%
  The precentage of types that didnt occur is 3.60288%

Question 4:
  Now replace singletons in the training data with <unk> symbol and map words (in the test corpus) not observed in training to <unk>. What percentage of bigrams (bigram types and bigram tokens) in the test corpus did not occur in training (treat <unk> as a regular token that has been observed). Please include the padding symbol </s> in your calculations. Do not include the start of sentence padding symbol <s>.

Answer 4:
  The percentage of tokens that didnt occur is: 37.614%
  The percentage of types that didn't occur is: 26.094%

Question 5:
  Compute the log probability of the following sentence under the three models (ignore capitalization and pad each sentence as described above). Please list all of the parameters required to compute the probabilities and show the complete calculation. Which
  of the parameters have zero values under each model? Use log base 2 in your calculations. Map words not observed in the training corpus to the <unk> token.
    â€¢ I look forward to hearing your reply .

Answer 5:
  The log probability of unigram: -87.90040262565967
  The log probability of bigram: N/A
  -> parameters with zero probability: ['hearing you', 'you reply', 'reply .']

  The log probability of bigram with smoothing: -122.07419306161714

Question 6:
Compute the perplexity of the sentence above under each of the models.

Answer 6:
  The perplexity of unigram:871.1101844415978
  The perplexity of bigram: N/A
  The perplexity of bigram with smoothing: 12109.059776900202 

Question 7:
Compute the perplexity of the entire test corpus under each of the models. Discuss the
differences in the results you obtained.

Answer 7:
  The unigram perplexity of the entire test corpus: 1097.1903087439796
  The bigram without smoothing of the entire test corpus: N/A
  -> Some of the bigrams were not found in the training data which is there is no perplexity for bigrams without smoothing.
  The bigram with smoothing of the entire test corpus: 12.066342508246969